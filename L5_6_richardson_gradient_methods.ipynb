{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOE2bw0LBgM5W2ZJqWEZRIW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdallas1/shared_code/blob/main/L5_6_richardson_gradient_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lfarfm5tEWeh",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!apt install octave"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Richardson and Gradient Methods\n",
        "\n",
        "We once again consider the problem of solving the system $A\\mathbf{x}=\\mathbf{b}$ with $A$ an $n\\times n$ matrix. Recall that last time we developed the general iterative method\n",
        "\n",
        "$$ \\mathbf{x}^{(k+1)} = (I-P^{-1}A)\\mathbf{x}^{(k)} + P^{-1}\\mathbf{b}. $$\n",
        "\n",
        "We also saw the fundamental theoretical result that says the sequence above converges to the solution $\\mathbf{x}^*$ if and only if $\\rho(I-P^{-1}A) < 1$ (note that this is really nothing more than Ostrowski's theorem). Defining the residual $\\mathbf{r}^{(k)} = \\mathbf{b} - A\\mathbf{x}^{(k)}$, we can express this write this iteration in the equivalent form\n",
        "\n",
        "$$ P\\mathbf{x}^{(k+1)} = P\\mathbf{x}^{(k)} + \\mathbf{r}^{(k)}.$$\n",
        "\n",
        "We also expressed this more algorithmically as\n",
        "\n",
        "1. Solve $P\\mathbf{z}^{(k)} = \\mathbf{r}^{(k)}$.\n",
        "\n",
        "2. Update: $\\mathbf{x}^{(k+1)}  = \\mathbf{x}^{(k)} + \\mathbf{z}^{(k)}$.\n",
        "\n",
        "Actually, the method we developed was\n",
        "\n",
        "1. Solve $P\\mathbf{z}^{(k)} = \\mathbf{r}^{(k)}$.\n",
        "\n",
        "2. Update: $\\mathbf{x}^{(k+1)}  = \\mathbf{x}^{(k)} + \\alpha_k\\mathbf{z}^{(k)}$.\n",
        "\n",
        "Here, $\\alpha_k$ is a scalar that is selected at each step to improve the convergence of the sequence $\\{\\mathbf{x}^{(k)}\\}$. The methods we saw last time, the Jacobi and Gauss-Seidel, both take $\\alpha_k = 1$ for all $k$. Our first objective in these notes is to learn how a nontrivial choice of $\\alpha_k$ affects convergence.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PervHGSgEc1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Static Richardson\n",
        "\n",
        "Let's first consider the case where we select $\\alpha_k = \\alpha \\neq 1$ for all $k$. Any such method is called a **stationary Richardson method** since $\\alpha$ is the same value at every iteration. We want to cast our iteration scheme with $\\alpha$ in the form $\\mathbf{x}^{(k+1)} = B\\mathbf{x}^{(k)}+\\mathbf{g}$, then we can use the convergence theory we saw last time. With some rearranging, we end up with\n",
        "\n",
        "$$ \\mathbf{x}^{(k+1)} = (I-\\alpha P^{-1}A)\\mathbf{x}^{(k)} + \\alpha P^{-1}\\mathbf{b}. $$\n",
        "\n",
        "Our iteration matrix is therefore $B = I - \\alpha P^{-1}A$. What follows is a discussion of how the results of Proposition 5.5 are obtained. This is just included for those that are curious. You may skip to Proposition 5.5 without worry.\n",
        "\n",
        "The fixed point iteration $ \\mathbf{x}^{(k+1)} = (I-\\alpha P^{-1}A)\\mathbf{x}^{(k)} + \\alpha P^{-1}\\mathbf{b}.$ converges if and only if $\\rho(I-\\alpha P^{-1}A) < 1$. Recall that $\\rho(B)$ is the largest eigenvalue (in magnitude) of $B$. Since we are free to $\\alpha$ however we like, it seems reasonable to choose $\\alpha$ to ensure that $\\rho(I-\\alpha P^{-1}A) < 1$. Note that\n",
        "\n",
        "$$ (I-\\alpha P^{-1}A)\\mathbf{v} = \\lambda \\mathbf{v} \\iff P^{-1}A\\mathbf{v} = \\dfrac{(1-\\lambda)}{\\alpha}\\mathbf{v}.$$\n",
        "\n",
        "In other words, $\\lambda$ is an eigenvalue of $I-\\alpha P^{-1}A$ if and only if $\\mu = (1-\\lambda)/\\alpha$ is an eigenvalue of $P^{-1}A$. Thus, every eigenvalue of $I-\\alpha P^{-1}A$ has the form $\\lambda = 1-\\alpha\\mu$, where $\\mu$ is an eigenvalue of $P^{-1}A$. Thus, we have convergence if and only if $|1-\\alpha\\mu| < 1$ for each eigenvalue $\\mu$ of $P^{-1}A$. If the eigenvalues $\\mu$ is complex valued, then $|1-\\alpha\\mu|^2 = (1-\\alpha\\mu)(1-\\alpha\\bar{\\mu}) = 1 -2\\alpha\\text{Re}\\mu +\\alpha^2 |\\mu|^2$. Thus\n",
        "\n",
        "$$ |1-\\alpha\\mu| < 1 \\iff |\\mu|^2 < 2\\dfrac{\\text{Re}\\mu}{\\alpha}.$$\n",
        "\n",
        "In the case where each eigenvalue $\\mu$ is real valued, this reduces to\n",
        "\n",
        "$$ 0 < \\alpha\\mu < 2.$$\n",
        "\n",
        "Notice that these conditions imply that the eigenvalues of $P^{-1}A$ must be all positive or all negative.\n",
        "\n",
        "Let's summarize these results in a proposition, and we'll also include some special results when $P$ and $A$ are SPD.\n",
        "\n",
        "> **Proposition 5.5** Let $A$ be an $n\\times n$ nonsingular matrix. For any nonsingular $n\\times n$ matrix $P$, the stationary Richardson iteration\n",
        "$$ \\mathbf{x}^{(k+1)} = (I-\\alpha P^{-1}A)\\mathbf{x}^{(k)} + \\alpha P^{-1}\\mathbf{b}$$\n",
        "converges if and only if\n",
        "$$ |\\mu|^2 \\leq \\dfrac{2}{\\alpha}\\text{Re}\\mu$$\n",
        "for each eigenvalue $\\mu$ of $P^{-1}A$.\n",
        "In the special case where $P$ and $A$ are SPD, then\n",
        "the stationary Richardson method converges if and only if $0 < \\alpha < 2/\\lambda_{max}$, where $\\lambda_{max}$ is the largest eigenvalue of $P^{-1}A$. Further, the spectral radius of $B_{\\alpha} = I - \\alpha P^{-1}A$ is minimized when\n",
        "$$ \\alpha = \\alpha_{opt} = \\dfrac{2}{\\lambda_{min}+\\lambda_{max}}$$\n",
        "where $\\lambda_{min}$ and $\\lambda_{max}$ are, respectively, the smallest and largest eigenvalues of $P^{-1}A$. With $\\alpha=\\alpha_{opt}$,\n",
        "$$ \\|\\mathbf{e}^{(k+1)}\\|_A \\leq \\left(\\dfrac{K(P^{-1}A)-1}{K(P^{-1}A)+1}\\right)^k\\|\\mathbf{e}^{(0)}\\|_A.$$\n",
        "Here, $\\|\\mathbf{v}\\|_A = \\sqrt{\\mathbf{v}^TA\\mathbf{v}}$ is the energy norm of $\\mathbf{v}$ associated with the matrix $A$.\n",
        "\n",
        "There are two big picture things to note here. First, as long as the eigenvalues of $P^{-1}A$ all have the same sign, we are guaranteed convergence as long as $\\alpha$ is sufficiently small. This is a very strong result. Further, when $P$ and $A$ are SPD, the best error bound we can obtain is\n",
        "\n",
        "$$ \\|\\mathbf{e}^{(k+1)}\\|_A \\leq \\left(\\dfrac{K(P^{-1}A)-1}{K(P^{-1}A)+1}\\right)^k\\|\\mathbf{e}^{(0)}\\|_A.$$\n",
        "\n",
        "This says that the closer $K(P^{-1}A)$ is to 1, or, equivalently, the better conditioned $P^{-1}A$ is, the faster stationary Richardson converges. This is precisely why $P$ is called a *preconditioner*. If $P$ is chosen wisely, $K(P^{-1}A)$ will be close to 1.\n",
        "\n",
        "These results are nice theoretically, but they are not necessarily the most practical. Computing the eigenvalues can be expensive, so we don't want to rely on them to inform our selection of $\\alpha$. We'll next consider how we can choose $\\alpha_k$ at each iteration to improve convergence without knowledge of the eigenvalues of $P^{-1}A$.\n"
      ],
      "metadata": {
        "id": "Tc32UMtiGUVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic Richardson\n",
        "\n",
        "For this section, let's suppose $A$ and $P$ are SPD. The following **dynamic Richardson method** is guaranteed to converge.\n",
        "\n",
        "1. Solve $P\\mathbf{z}^{(k)} = \\mathbf{r}^{(k)}$.\n",
        "\n",
        "2. Set $\\alpha_k = \\dfrac{(\\mathbf{z}^{(k)})^T\\mathbf{r}^{(k)}}{(\\mathbf{z}^{(k)})^TA\\mathbf{z}^{(k)}}$.\n",
        "\n",
        "3. Update $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\alpha_k\\mathbf{z}^{(k)}$.\n",
        "\n",
        "4. Update $\\mathbf{r}^{(k+1)} = \\mathbf{r}^{(k)} - \\alpha_k A\\mathbf{z}^{(k)}$.\n",
        "\n",
        "This is known as the **preconditioned gradient method**, or simply the **gradient method** if $P=I$. Further, we have the error bound\n",
        "\n",
        "$$ \\|\\mathbf{e}^{(k+1)}\\|_A \\leq \\left(\\dfrac{K(P^{-1}A)-1}{K(P^{-1}A)+1}\\right)^k\\|\\mathbf{e}^{(0)}\\|_A.$$\n",
        "\n",
        "To motivate this choice of $\\alpha_k$, and provide some intuition as to why it's a good choice, we'll take a brief detour into numerical optimization.\n",
        "\n",
        "Define $f(\\mathbf{x})$ to be the multivariate function\n",
        "\n",
        "$$f(\\mathbf{x}) = \\dfrac{1}{2}\\mathbf{x}^TA\\mathbf{x} - \\mathbf{x}^T\\mathbf{b} + \\mathbf{c}.$$\n",
        "\n",
        "Any function of this form is called a **quadratic form.** The code cell below plots the quadratic form defined by\n",
        "\n",
        "$$ A = \\begin{bmatrix} 10 &1 \\\\ 1 &11\\end{bmatrix},\\hspace{1em} \\mathbf{b}=\\begin{bmatrix} 1 \\\\2 \\end{bmatrix}.$$\n",
        "\n",
        "Inspection shows that $A$ is symmetric, and you can verify that $A$ is positive definite by showing that the determinant of $A$ is positive and the trace (sum of diagonal entries) is positive. It follows that all eigenvalues are positive, which is equivalent to $A$ being positive definite when $A$ is symmetric.\n"
      ],
      "metadata": {
        "id": "RAz0bIm9fkwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "A = np.array([[10,1],[1,11]])\n",
        "b = np.array([1,2])\n",
        "\n",
        "def quad_form(x,y):\n",
        "  return 0.5*np.transpose(np.array([x,y])) @ A @ np.array([x,y]) - np.transpose(np.array([x,y])) @ b;\n",
        "\n",
        "x = y = np.arange(-5,5,0.1)\n",
        "X,Y = np.meshgrid(x,y)\n",
        "Z = np.array([quad_form(x,y) for x,y in zip(np.ravel(X),np.ravel(Y))])\n",
        "Z = np.resize(Z,X.shape)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X,Y,Z)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FnAxSaw_Be0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot from the cell above shows that $f(\\mathbf{x})$ is a paraboloid, and therefore has a unique minimizer $\\mathbf{x}^*$.  \n",
        "\n",
        "This minimizer is the unique solution of $A\\mathbf{x}=\\mathbf{b}$ when $A$ is SPD by first order optimality conditions. Indeed, you can verify that\n",
        "\n",
        "$$ \\nabla f(\\mathbf{x}) = A\\mathbf{x} - \\mathbf{b}.$$\n",
        "\n",
        "Since the minimizer $\\mathbf{x}^*$ must be a root of the gradient of $f$, it follows that $A\\mathbf{x}^*=\\mathbf{b}$. The point is that when $A$ is SPD, solving the linear system $A\\mathbf{x}=\\mathbf{b}$ is equivalent to minimizing the quadratic form $f(\\mathbf{x})$. This observation will motivate the choice of $\\alpha_k$ in the dynamic Richardson algorithm above.\n",
        "\n",
        "Let's pivot to minimizing $f(\\mathbf{x})$. One of the most fundamental numerical optimization techniques is the **gradient descent method**. Since $\\nabla f(\\mathbf{x})$ tells us the direct of steepest *ascent*, $-\\nabla f(\\mathbf{x})$ is the direction of steepest *descent*. So, if we have an approximate minimizer $\\mathbf{x}^{(k)}$, if we take a step from\n",
        "$\\mathbf{x}^{(k)}$ along $-\\nabla f(\\mathbf{x}^{(k)})$ will decrease $f$. We thus obtain the sequence\n",
        "\n",
        "$$ \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\alpha_k\\nabla f(\\mathbf{x}^{(k)}).$$\n",
        "\n",
        "Notice that $-\\nabla f(\\mathbf{x}^{(k)}) = \\mathbf{b}-A\\mathbf{x}^{(k)}$. So the steepest descent, or gradient, step is equivalent to\n",
        "\n",
        "$$ \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\alpha_k\\mathbf{r}^{(k)}.$$\n",
        "\n",
        "This is exactly what we have above if $P=I$. Now, let's choose $\\alpha_k$ to solve\n",
        "\n",
        "$$ \\min_{\\alpha} f(\\mathbf{x}^{(k)} + \\alpha\\mathbf{r}^{(k)}) .$$\n",
        "\n",
        "Intuitively, we've found a step that will decrease $f$, but now we're going to select $\\alpha_k$ so that we reach the lowest point of $f$ possible along the direction $-\\nabla f(\\mathbf{x}^{(k)})$. Now let's solve $\\min f(\\mathbf{x}^{(k)} + \\alpha\\mathbf{r}^{(k)}).$  \n",
        "\n",
        "$$ g(\\alpha) = f(\\mathbf{x}^{(k)}+\\alpha \\mathbf{r}^{(k)}) \\implies g'(\\alpha) = \\nabla f(\\mathbf{x}^{(k)} + \\alpha \\mathbf{r}^{(k)})^T\\mathbf{r}^{(k)}.$$\n",
        "\n",
        "Since $\\alpha_k$ minimized $g(\\alpha)$, $g'(\\alpha_k)=0$. Hence\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\nabla f(\\mathbf{x}^{(k)} + \\alpha_k \\mathbf{r}^{(k)})^T\\mathbf{r}^{(k)} &= \\left( A\\mathbf{x}^{(k)} +\\alpha_kAr^{(k)} - \\mathbf{b}\\right)^T\\mathbf{r}^{(k)}\\\\\n",
        " &= (-\\mathbf{r}^{(k)}+\\alpha_k A\\mathbf{r}^{(k)})^T\\mathbf{r}^{(k)} = 0 \\\\\n",
        " &\\iff\\\\\n",
        " \\alpha_k &= \\dfrac{(\\mathbf{r}^{(k)})^T\\mathbf{r}^{(k)}}{(\\mathbf{r}^{(k)})^TA\\mathbf{r}^{(k)}}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "If $P=I$, this $\\alpha_k$ is precisely what we used above in the dynamic Richardson iteration. Going back through the steps taken to derive $\\alpha_k$, but with $P^{-1}\\mathbf{r}^{(k)} = \\mathbf{z}^{(k)}$ instead of $\\mathbf{r}^{(k)}$ yields\n",
        "\n",
        "$$  \\alpha_k = \\dfrac{(\\mathbf{z}^{(k)})^T\\mathbf{r}^{(k)}}{(\\mathbf{z}^{(k)})^TA\\mathbf{z}^{(k)}}$$\n",
        "\n",
        "which corresponds to the preconditioned steepest descent method. Remarkably, this choice of $\\alpha_k$ also minimizes the error with respect to the energy norm. Precisely, for any real number $\\alpha$,\n",
        "\n",
        "$$ \\|\\mathbf{x}^* - \\mathbf{x}^{k+1}\\|_A \\leq \\|\\mathbf{x}^* - (\\mathbf{x}^{(k)} + \\alpha \\mathbf{z}^{(k)})\\|_A $$\n",
        "\n",
        "where $\\mathbf{x}^{k+1} = \\mathbf{x}^{(k)} + \\alpha_k \\mathbf{z}^{(k)}$.\n"
      ],
      "metadata": {
        "id": "obxqNhVjBXDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conjugate Gradient Method\n",
        "\n",
        "Our goal in this section is once again to solve $A\\mathbf{x}=\\mathbf{b}$, where $A$ is a square SPD matrix. In the steepest descent, or dynamic Richardson, method, we choose $\\alpha_k$ to ensure that the residual at step $k+1$ is orthogonal to the previous update direction (descent direction) $\\mathbf{z}^{(k)}$. This can yield fast convergence, but can also lead to inefficient \"zig zagging.\" We can take a more direct route to the solution by instead choosing our update steps to be $A$-orthogonal. This means they are orthogonal with resepct to the energy inner product:\n",
        "$(\\mathbf{u},\\mathbf{v})_A = \\mathbf{u}^TA\\mathbf{v}$. Another term for $A$-orthgonal is $A$-conjugate. Hence the name of the Conjugate Gradient (CG) Method.\n",
        "\n",
        "We will denote the $A$-orthogonal descent direction at step $k$ by $\\mathbf{p}^{(k)}$. The goal is to generate a sequence of descent directions $\\{\\mathbf{p}^{(k)}\\}$ such that\n",
        "\n",
        "$$(\\mathbf{p}^{(i)})^TA\\mathbf{p}^{(j)} = \\delta_{ij}.$$\n",
        "\n",
        "Let's see how we might do this. The first step is up to us. Let's take $\\mathbf{p}^{(0)} = \\mathbf{r}^{(0)}$. Then choosing $\\alpha_k$ the same as before, we get\n",
        "\n",
        "$$\n",
        "\\begin{align} \\alpha_0 &= \\dfrac{(\\mathbf{p}^{(0)})^T\\mathbf{r}^{(0)}}{(\\mathbf{p}^{(0)})^TA\\mathbf{p}^{(0)}}\\\\\n",
        "\\mathbf{x}^{(1)} &= \\mathbf{x}^{(0)} + \\alpha_0\\mathbf{p}^{(0)}\\\\\n",
        "\\mathbf{r}^{(1)} &= \\mathbf{r}^{(0)} - \\alpha_0A\\mathbf{p}^{(0)}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Now, how do we select $\\mathbf{p}^{(1)}$? If we take $\\mathbf{r}^{(1)}$, and subtract the component of $\\mathbf{r}^{(1)}$ along $\\mathbf{p}^{(0)}$ with respect to the energy inner-product, we'll be left with a descent direction that is $A$-orthogonal to $\\mathbf{p}^{(0)}$, just like we want. We thus obtain\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\beta_0 &= \\dfrac{(\\mathbf{p}^{(0)})^TA\\mathbf{r}^{(1)}}{(\\mathbf{p}^{(0)})^TA\\mathbf{p}^{(0)}}\\\\\n",
        "\\mathbf{p}^{(1)} &= \\mathbf{r}^{(1)} - \\beta_0\\mathbf{p}^{(0)}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "This is nothing more than the Gram-Schmidt process, but with respect to the energy inner-product rather than the usual dot product. However, it can be proven with induction, that as long as $\\mathbf{p}^{(k+1)}$ is $A$-orthogonal to $\\mathbf{p}^{(k)}$, then  $\\mathbf{p}^{(k+1)}$ is automatically orthogonal to $\\mathbf{p}^{(j)}$ for $j=0,...,k-1$. This is a remarkable fact, and makes CG that much more efficient. We have thus obtained the following CG method.\n",
        "\n",
        "For $k=0,1,...$\n",
        "\n",
        "1. Set $\\alpha_k = \\dfrac{(\\mathbf{p}^{(k)})^T\\mathbf{r}^{(k)}}{(\\mathbf{p}^{(k)})^TA\\mathbf{p}^{(k)}}$.\n",
        "\n",
        "2. Update $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\alpha_k\\mathbf{p}^{(k)}$.\n",
        "\n",
        "3. Update $\\mathbf{r}^{(k+1)} = \\mathbf{r}^{(k)} - \\alpha_kA\\mathbf{p}^{(k)}$.\n",
        "\n",
        "4. Set $\\beta_k = \\dfrac{(\\mathbf{p}^{(k)})^TA\\mathbf{r}^{(k+1)}}{(\\mathbf{p}^{(k)})^TA\\mathbf{p}^{(k)}}$.\n",
        "\n",
        "5. Update $\\mathbf{p}^{(k+1)} = \\mathbf{r}^{(k+1)} - \\beta_k\\mathbf{p}^{(k)}$\n",
        "\n",
        "In step 1, we choose $\\alpha_k$ to minimize $\\|\\mathbf{e}^{(k+1)}\\|_A$. In step 4, we select $\\beta_k$ to ensure that $\\mathbf{p}^{(k+1)}$ is $A$-orthgonal to $\\mathbf{p}^{(k)}$.\n",
        "\n",
        "The following is a fundamental result for the CG method, and is found on page 178 of our book.\n",
        "\n",
        "> **Proposition 5.7** Let $A$ be an $n\\times n$ SPD matrix. In exact arithmetic, the CG method converges in at most $n$ steps. Further, the error at step $k < n$ is orthogonal to $\\mathbf{p}^{(j)}$ for $j=0,...,k-1$, and it satisfies\n",
        "$$ \\|\\mathbf{e}^{(k)}\\|_A \\leq \\dfrac{2c^k}{1+c^{2k}}\\|\\mathbf{e}^{(0)}\\|_A$$\n",
        "where $c = \\dfrac{\\sqrt{K(A)}-1}{\\sqrt{K(A)}+1}.$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4j7srW_RsrSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preconditioned Conjugate Gradient (PCG)\n",
        "\n",
        "We've seen that preconditioning can improve the convergence properties of an iterative method. This is true for the CG method as well. The **preconditioned conjugate gradient**, denoted PCG, is very similar to CG except for one step. At step $k$, before computing the next descent direction $\\mathbf{p}^{(k+1)}$, we solve the system $P\\mathbf{z}^{(k+1)} = \\mathbf{r}^{(k+1)}$.\n",
        "\n",
        "For $k=0,1,...$\n",
        "\n",
        "1. Set $\\alpha_k = \\dfrac{(\\mathbf{p}^{(k)})^T\\mathbf{r}^{(k)}}{(\\mathbf{p}^{(k)})^TA\\mathbf{p}^{(k)}}$.\n",
        "\n",
        "2. Update $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\alpha_k\\mathbf{p}^{(k)}$.\n",
        "\n",
        "3. Update $\\mathbf{r}^{(k+1)} = \\mathbf{r}^{(k)} - \\alpha_kA\\mathbf{p}^{(k)}$.\n",
        "\n",
        "4. Solve $P\\mathbf{z}^{(k+1)} = \\mathbf{r}^{(k+1)}$.\n",
        "\n",
        "5. Set $\\beta_k = \\dfrac{(\\mathbf{p}^{(k)})^TA\\mathbf{z}^{(k+1)}}{(\\mathbf{p}^{(k)})^TA\\mathbf{p}^{(k)}}$.\n",
        "\n",
        "6. Update $\\mathbf{p}^{(k+1)} = \\mathbf{r}^{(k+1)} - \\beta_k\\mathbf{p}^{(k)}$"
      ],
      "metadata": {
        "id": "lYyGtKwD6_vl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implemenations"
      ],
      "metadata": {
        "id": "_svSoYtAuY_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preconditioned Gradient\n",
        "%%writefile pg.m\n",
        "function [x,iters] = pg(A,b,tol,maxiters,P)\n",
        "  % ====================================\n",
        "  % SOLVES THE SYSTEM Ax = b USING\n",
        "  % PRECONDITIONED GRADIENT.\n",
        "  % TOL IS THE RELATIVE ERROR TOLERANCE.\n",
        "  % MAXITERS IS THE MAXIMUM NUMBER OF\n",
        "  % ITERATIONS.\n",
        "  % P IS THE PRECONDITIONER.\n",
        "  % ====================================\n",
        "  n = length(b); x = zeros(n,1); r = b - A*x;\n",
        "  iters = 0; r0 = r;\n",
        "  while norm(r) > tol && iters < maxiters\n",
        "    z = inv(P)*r;\n",
        "    alpha = dot(r,z)/dot(A*z,z);\n",
        "    x = x + alpha * z;\n",
        "    r = r - alpha * A * z;\n",
        "    iters += 1;\n",
        "    err = norm(r)/norm(b);\n",
        "  end"
      ],
      "metadata": {
        "id": "3JH6t3YGmLYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Conjugate Gradient algorithm\n",
        "%%writefile cg.m\n",
        "\n",
        "function [x,iters] = cg(A,b,tol,maxiters,P)\n",
        "  % ========================================\n",
        "  % SOLVES THE SYSTEM Ax = b USING CG.\n",
        "  % TOL IS THE RELATIVE ERROR TOLERANCE.\n",
        "  % MAXITERS IS THE MAXIMUM NUMBER OF\n",
        "  % ITERATIONS.\n",
        "  % P IS THE PRECONDITIONER.\n",
        "  % ========================================\n",
        "  n = length(b); x = zeros(n,1); r = b - A*x;\n",
        "  iters = 0; z = inv(P)*r; p = z;\n",
        "  while norm(r) > tol && iters < maxiters\n",
        "    alpha = dot(p,r)/dot(A*p,p);\n",
        "    x = x + alpha * p;\n",
        "    r = r - alpha * A * p;\n",
        "    z = inv(P)*r;\n",
        "    beta = dot(A*p,z)/dot(A*p,p);\n",
        "    p = z - beta * p;\n",
        "    iters += 1;\n",
        "    err = norm(r)/norm(b);\n",
        "  end\n",
        "\n"
      ],
      "metadata": {
        "id": "jI0nSZ09fzUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example 5.16\n",
        "%%writefile ex5_16.m\n",
        "\n",
        "% =============================\n",
        "% COMPARE BACKLASH, CG, AND PCG\n",
        "% TO SOLVE THE HILBERT SYSTEM\n",
        "% =============================\n",
        "\n",
        "% SET SIZE OF SYSTEM.\n",
        "N = 20;\n",
        "\n",
        "% SET ITERATIVE PARAMETERS\n",
        "tol = 1e-6; err_cg = 1 + tol; err_pcg = 1 + tol;\n",
        "maxiters = 1e4; alpha=0; beta = 0; iters_cg = 0; iters_pcg = 0;\n",
        "\n",
        "fprintf(\"N | Condition Number | Backslash | PG \\t         | PCG\\n\")\n",
        "fprintf(\"==============================================================\\n\")\n",
        "\n",
        "for n = 4:2:14\n",
        "  % CONSTRUCT HILBERT MATRIX H\n",
        "  v = 0:n-1 ;\n",
        "  H = 1 ./ (v + [1:n]') ;\n",
        "\n",
        "  % SET EXACT SOLUTION AND RIGHT HAND SIDE\n",
        "  x_true = ones(n,1); b = H*x_true;\n",
        "\n",
        "  % SOLVE USING BACKLASH (USES CHOLESKY)\n",
        "  x_bckslash = H \\ b;\n",
        "\n",
        "  % COMPUTE RELATIVE ERROR FOR BACKSLASH\n",
        "  err_bckslash = norm(x_bckslash - x_true)/norm(x_true);\n",
        "\n",
        "  % SOLVE USING PG\n",
        "  [x_pg,iters_pg] = pg(H,b,tol,maxiters,diag(diag(H)));\n",
        "  err_pg = norm(x_pg - x_true)/norm(x_true);\n",
        "\n",
        "  % SOLVE USING PCG\n",
        "  [x_pcg,iters_pcg] = cg(H,b,tol,maxiters,diag(diag(H)));\n",
        "  err_pcg = norm(x_pcg - x_true)/norm(x_true);\n",
        "\n",
        "  K = cond(H);\n",
        "  fprintf(\"%g | %0.1e\\t     | %0.1e   | %0.1e %g | %0.1e %g \\n\",n,K,err_bckslash,err_pg,iters_pg,err_pcg,iters_pcg);\n",
        "end\n"
      ],
      "metadata": {
        "id": "4QUHN1bC-GpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!octave -W ex5_16.m"
      ],
      "metadata": {
        "id": "EYQwerwGDbBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Iterative Methods\n",
        "\n",
        "1. An iterative method generates a sequence $\\{\\mathbf{x}^{(k)}\\}$ where $\\mathbf{x}^{(k)}\\to\\mathbf{x}^*$ as $k\\to\\infty$, and $\\mathbf{x}^*$ solves the system $A\\mathbf{x}=\\mathbf{b}$.\n",
        "\n",
        "2. Iterative methods can be more computationally efficient compared to direct methods like the LU method, and are much more efficient when a solution is desired up to a tolerance less than machine epsilon.\n",
        "\n",
        "3. The most general iterative scheme takes an initial guess $\\mathbf{x}^{(0)}$, and for $k\\geq 0$,\n",
        "\n",
        "$$ \\mathbf{x}^{(k+1)} = B\\mathbf{x}^{(k)}+\\mathbf{g}.$$\n",
        "\n",
        "This sequence converges to the solution $\\mathbf{x}^*$ of $A\\mathbf{x}=\\mathbf{b}$ provided that\n",
        "\n",
        "$$ \\mathbf{x}^* = B\\mathbf{x}^*+\\mathbf{g}$$\n",
        "\n",
        "and $\\rho(B) < 1$.\n",
        "\n",
        "4. The Jacobi and Gauss-Seidel methods are two classical iterative methods. The Jacobi method converges as if $A$ is stricly diagonally dominant by row, and Gauss-Seidel converges if $A$ is SPD or strictly diagonally dominant by row.\n",
        "\n",
        "5. In general, neither Jacobi nor Gauss-Seidel is better. Depending on the system, one method may converge faster than the other, but in the case of a tridiagonal system, if Jacobi converges, so does Gauss-Seidel, and Gauss-Seidel converges faster. However, the Jacobi method is highly parallelizable, while Gauss-Seidel is not.\n",
        "\n",
        "6. The Richardson method can converge faster than classical methods by choosing the parameter $\\alpha$ and preconditioning matrix appropriately. The parameter $\\alpha$ should be chosen based on the eigenvalues of $P^{-1}A$, which can be difficult to compute in general. There is no \"formula\" for determining the preconditioning matrix $P$. It is typically chosen based on the properties of $A$.\n",
        "\n",
        "7. Dynamic Richardson methods, like the Gradient and Conjugate Gradient methods, avoid the problem of needing to know the eigenvalues of $P^{-1}A$, since $\\alpha_k$ is computed at each step using known quantities. Conjugate Gradient Preconditioned Conjugate Gradient work very well for SPD matrices, and generalizations exist for nonsymmetric matrices. (Krylov methods. *Great* final project idea.)\n",
        "\n",
        "8. When selecting stopping criteria for an iterative scheme, one can choose to terminate the iteration once the difference between consecutive iterates is sufficiently small, or if the residual is sufficiently small. See page 180 in the book for more details on this."
      ],
      "metadata": {
        "id": "_83VXIiJvGDr"
      }
    }
  ]
}