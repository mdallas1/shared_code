{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef41d529-3502-42f2-b082-90cb45f2e5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Total Loss: 0.767286, Lambda_Y: 0.01900\n",
      "Epoch 500, Total Loss: 0.000010, Lambda_Y: 0.00606\n",
      "Epoch 1000, Total Loss: 0.000001, Lambda_Y: 0.00637\n",
      "Epoch 1500, Total Loss: 0.000000, Lambda_Y: 0.00641\n",
      "Epoch 2000, Total Loss: 0.000000, Lambda_Y: 0.00642\n",
      "Epoch 2500, Total Loss: 0.000000, Lambda_Y: 0.00643\n",
      "Epoch 3000, Total Loss: 0.000000, Lambda_Y: 0.00644\n",
      "Epoch 3500, Total Loss: 0.000001, Lambda_Y: 0.00644\n",
      "Epoch 4000, Total Loss: 0.000003, Lambda_Y: 0.00645\n",
      "Epoch 4500, Total Loss: 0.000000, Lambda_Y: 0.00645\n",
      "Predictions (Y-88, Sr-88):\n",
      "Time: 0.00 days, Y-88: 1.0023, Sr-88: -0.0023\n",
      "Time: 2.02 days, Y-88: 0.9866, Sr-88: 0.0134\n",
      "Time: 4.04 days, Y-88: 0.9738, Sr-88: 0.0262\n",
      "Time: 6.06 days, Y-88: 0.9619, Sr-88: 0.0381\n",
      "Time: 8.08 days, Y-88: 0.9499, Sr-88: 0.0501\n",
      "Time: 10.10 days, Y-88: 0.9372, Sr-88: 0.0628\n",
      "Time: 12.12 days, Y-88: 0.9243, Sr-88: 0.0757\n",
      "Time: 14.14 days, Y-88: 0.9118, Sr-88: 0.0882\n",
      "Time: 16.16 days, Y-88: 0.8998, Sr-88: 0.1002\n",
      "Time: 18.18 days, Y-88: 0.8882, Sr-88: 0.1118\n",
      "Time: 20.20 days, Y-88: 0.8768, Sr-88: 0.1232\n",
      "Time: 22.22 days, Y-88: 0.8656, Sr-88: 0.1344\n",
      "Time: 24.24 days, Y-88: 0.8545, Sr-88: 0.1455\n",
      "Time: 26.26 days, Y-88: 0.8434, Sr-88: 0.1566\n",
      "Time: 28.28 days, Y-88: 0.8324, Sr-88: 0.1676\n",
      "Time: 30.30 days, Y-88: 0.8216, Sr-88: 0.1784\n",
      "Time: 32.32 days, Y-88: 0.8108, Sr-88: 0.1892\n",
      "Time: 34.34 days, Y-88: 0.8001, Sr-88: 0.1999\n",
      "Time: 36.36 days, Y-88: 0.7896, Sr-88: 0.2104\n",
      "Time: 38.38 days, Y-88: 0.7792, Sr-88: 0.2208\n",
      "Time: 40.40 days, Y-88: 0.7689, Sr-88: 0.2311\n",
      "Time: 42.42 days, Y-88: 0.7588, Sr-88: 0.2412\n",
      "Time: 44.44 days, Y-88: 0.7488, Sr-88: 0.2512\n",
      "Time: 46.46 days, Y-88: 0.7390, Sr-88: 0.2610\n",
      "Time: 48.48 days, Y-88: 0.7293, Sr-88: 0.2707\n",
      "Time: 50.51 days, Y-88: 0.7198, Sr-88: 0.2802\n",
      "Time: 52.53 days, Y-88: 0.7104, Sr-88: 0.2896\n",
      "Time: 54.55 days, Y-88: 0.7011, Sr-88: 0.2989\n",
      "Time: 56.57 days, Y-88: 0.6920, Sr-88: 0.3080\n",
      "Time: 58.59 days, Y-88: 0.6831, Sr-88: 0.3169\n",
      "Time: 60.61 days, Y-88: 0.6742, Sr-88: 0.3258\n",
      "Time: 62.63 days, Y-88: 0.6655, Sr-88: 0.3345\n",
      "Time: 64.65 days, Y-88: 0.6569, Sr-88: 0.3431\n",
      "Time: 66.67 days, Y-88: 0.6485, Sr-88: 0.3515\n",
      "Time: 68.69 days, Y-88: 0.6401, Sr-88: 0.3599\n",
      "Time: 70.71 days, Y-88: 0.6319, Sr-88: 0.3681\n",
      "Time: 72.73 days, Y-88: 0.6238, Sr-88: 0.3762\n",
      "Time: 74.75 days, Y-88: 0.6157, Sr-88: 0.3843\n",
      "Time: 76.77 days, Y-88: 0.6078, Sr-88: 0.3922\n",
      "Time: 78.79 days, Y-88: 0.6000, Sr-88: 0.4000\n",
      "Time: 80.81 days, Y-88: 0.5923, Sr-88: 0.4077\n",
      "Time: 82.83 days, Y-88: 0.5847, Sr-88: 0.4153\n",
      "Time: 84.85 days, Y-88: 0.5771, Sr-88: 0.4229\n",
      "Time: 86.87 days, Y-88: 0.5697, Sr-88: 0.4303\n",
      "Time: 88.89 days, Y-88: 0.5623, Sr-88: 0.4377\n",
      "Time: 90.91 days, Y-88: 0.5550, Sr-88: 0.4450\n",
      "Time: 92.93 days, Y-88: 0.5478, Sr-88: 0.4522\n",
      "Time: 94.95 days, Y-88: 0.5407, Sr-88: 0.4593\n",
      "Time: 96.97 days, Y-88: 0.5337, Sr-88: 0.4663\n",
      "Time: 98.99 days, Y-88: 0.5267, Sr-88: 0.4733\n",
      "Time: 101.01 days, Y-88: 0.5199, Sr-88: 0.4801\n",
      "Time: 103.03 days, Y-88: 0.5131, Sr-88: 0.4869\n",
      "Time: 105.05 days, Y-88: 0.5063, Sr-88: 0.4937\n",
      "Time: 107.07 days, Y-88: 0.4997, Sr-88: 0.5003\n",
      "Time: 109.09 days, Y-88: 0.4931, Sr-88: 0.5069\n",
      "Time: 111.11 days, Y-88: 0.4866, Sr-88: 0.5134\n",
      "Time: 113.13 days, Y-88: 0.4802, Sr-88: 0.5198\n",
      "Time: 115.15 days, Y-88: 0.4738, Sr-88: 0.5262\n",
      "Time: 117.17 days, Y-88: 0.4676, Sr-88: 0.5324\n",
      "Time: 119.19 days, Y-88: 0.4614, Sr-88: 0.5386\n",
      "Time: 121.21 days, Y-88: 0.4552, Sr-88: 0.5448\n",
      "Time: 123.23 days, Y-88: 0.4492, Sr-88: 0.5508\n",
      "Time: 125.25 days, Y-88: 0.4432, Sr-88: 0.5568\n",
      "Time: 127.27 days, Y-88: 0.4373, Sr-88: 0.5627\n",
      "Time: 129.29 days, Y-88: 0.4315, Sr-88: 0.5685\n",
      "Time: 131.31 days, Y-88: 0.4257, Sr-88: 0.5743\n",
      "Time: 133.33 days, Y-88: 0.4200, Sr-88: 0.5800\n",
      "Time: 135.35 days, Y-88: 0.4144, Sr-88: 0.5856\n",
      "Time: 137.37 days, Y-88: 0.4089, Sr-88: 0.5911\n",
      "Time: 139.39 days, Y-88: 0.4034, Sr-88: 0.5966\n",
      "Time: 141.41 days, Y-88: 0.3981, Sr-88: 0.6019\n",
      "Time: 143.43 days, Y-88: 0.3928, Sr-88: 0.6072\n",
      "Time: 145.45 days, Y-88: 0.3876, Sr-88: 0.6124\n",
      "Time: 147.47 days, Y-88: 0.3824, Sr-88: 0.6176\n",
      "Time: 149.49 days, Y-88: 0.3774, Sr-88: 0.6226\n",
      "Time: 151.52 days, Y-88: 0.3724, Sr-88: 0.6276\n",
      "Time: 153.54 days, Y-88: 0.3675, Sr-88: 0.6325\n",
      "Time: 155.56 days, Y-88: 0.3626, Sr-88: 0.6374\n",
      "Time: 157.58 days, Y-88: 0.3579, Sr-88: 0.6421\n",
      "Time: 159.60 days, Y-88: 0.3532, Sr-88: 0.6468\n",
      "Time: 161.62 days, Y-88: 0.3486, Sr-88: 0.6514\n",
      "Time: 163.64 days, Y-88: 0.3441, Sr-88: 0.6559\n",
      "Time: 165.66 days, Y-88: 0.3397, Sr-88: 0.6603\n",
      "Time: 167.68 days, Y-88: 0.3354, Sr-88: 0.6646\n",
      "Time: 169.70 days, Y-88: 0.3311, Sr-88: 0.6689\n",
      "Time: 171.72 days, Y-88: 0.3269, Sr-88: 0.6731\n",
      "Time: 173.74 days, Y-88: 0.3228, Sr-88: 0.6772\n",
      "Time: 175.76 days, Y-88: 0.3187, Sr-88: 0.6813\n",
      "Time: 177.78 days, Y-88: 0.3148, Sr-88: 0.6852\n",
      "Time: 179.80 days, Y-88: 0.3109, Sr-88: 0.6891\n",
      "Time: 181.82 days, Y-88: 0.3071, Sr-88: 0.6929\n",
      "Time: 183.84 days, Y-88: 0.3033, Sr-88: 0.6967\n",
      "Time: 185.86 days, Y-88: 0.2997, Sr-88: 0.7003\n",
      "Time: 187.88 days, Y-88: 0.2961, Sr-88: 0.7039\n",
      "Time: 189.90 days, Y-88: 0.2926, Sr-88: 0.7074\n",
      "Time: 191.92 days, Y-88: 0.2891, Sr-88: 0.7109\n",
      "Time: 193.94 days, Y-88: 0.2858, Sr-88: 0.7142\n",
      "Time: 195.96 days, Y-88: 0.2825, Sr-88: 0.7175\n",
      "Time: 197.98 days, Y-88: 0.2793, Sr-88: 0.7207\n",
      "Time: 200.00 days, Y-88: 0.2761, Sr-88: 0.7239\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the PINN model\n",
    "class PINN_Y88(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINN_Y88, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.lambda_Y = nn.Parameter(torch.tensor(0.02, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.net(t)\n",
    "\n",
    "# Physics-based loss: dY/dt = -lambda * Y\n",
    "def physics_loss(model, t):\n",
    "    t.requires_grad = True\n",
    "    Y = model(t)\n",
    "    dY_dt = torch.autograd.grad(\n",
    "        outputs=Y,\n",
    "        inputs=t,\n",
    "        grad_outputs=torch.ones_like(Y),\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "    lambda_Y = model.lambda_Y\n",
    "    return torch.mean((dY_dt + lambda_Y * Y)**2)\n",
    "\n",
    "# Training data: time values and known Y(t)\n",
    "lambda_true = 0.0065\n",
    "t_train = torch.linspace(0, 200, 200).view(-1, 1).float()\n",
    "Y_true = torch.exp(-lambda_true * t_train)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = PINN_Y88()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    Y_pred = model(t_train)\n",
    "\n",
    "    # Data loss: match known decay curve\n",
    "    data_loss = torch.mean((Y_pred - Y_true)**2)\n",
    "\n",
    "    # Physics loss: match the ODE\n",
    "    p_loss = physics_loss(model, t_train)\n",
    "\n",
    "    # Total loss\n",
    "    loss = data_loss + p_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Total Loss: {loss.item():.6f}, Lambda_Y: {model.lambda_Y.item():.5f}\")\n",
    "\n",
    "# Evaluate and plot\n",
    "model.eval()\n",
    "t_test = torch.linspace(0, 200, 100).view(-1, 1).float()\n",
    "with torch.no_grad():\n",
    "    Y_test = model(t_test).cpu().numpy()\n",
    "    Sr_test = 1 - Y_test\n",
    "\n",
    "# Print predictions\n",
    "print(\"Predictions (Y-88, Sr-88):\")\n",
    "for i, time in enumerate(t_test.numpy().flatten()):\n",
    "    print(f\"Time: {time:.2f} days, Y-88: {Y_test[i, 0]:.4f}, Sr-88: {Sr_test[i, 0]:.4f}\")\n",
    "\n",
    "# t_np = t_test.numpy().flatten()\n",
    "# plt.plot(t_np, Y_test[:, 0], label=\"Y-88 (PINN)\")\n",
    "# plt.plot(t_np, Sr_test[:, 0], label=\"Sr-88 (PINN)\")\n",
    "# plt.xlabel(\"Time (days)\")\n",
    "# plt.ylabel(\"Amount (mol)\")\n",
    "# plt.title(\"Y-88 Decay and Sr-88 Accumulation (PINN)\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
